from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from datetime import datetime
import time
import warnings
import logging
from urllib.parse import urlparse  # âœ… NEW
from typing import Union

# ğŸ”‡ Táº¯t hoÃ n toÃ n cÃ¡c warning vÃ  log khÃ´ng cáº§n thiáº¿t
warnings.filterwarnings("ignore")
logging.getLogger('urllib3').setLevel(logging.ERROR)
logging.getLogger('selenium').setLevel(logging.ERROR)

# Import centralized database system
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from database import SupabaseManager, DatabaseConfig, format_datetime_for_db

# Constants
STOCK_CODES = ["FPT", "GAS", "IMP", "VCB"]

# Helper functions
def get_database_manager():
    """Get database manager instance"""
    return SupabaseManager()

def get_table_name(stock_code=None, is_general=False):
    """Get table name using new config"""
    config = DatabaseConfig()
    return config.get_table_name(stock_code=stock_code, is_general=is_general)

def get_recent_links_from_db(db_manager, table_name, limit=100):
    """Láº¥y 100 link bÃ i viáº¿t gáº§n nháº¥t tá»« database cho keyword crawling"""
    try:
        supabase_client = db_manager.get_supabase_client()
        # Thá»­ dÃ¹ng id thay vÃ¬ created_at náº¿u khÃ´ng cÃ³ created_at
        try:
            result = supabase_client.table(table_name).select("link").order("created_at", desc=True).limit(limit).execute()
        except Exception:
            try:
                result = supabase_client.table(table_name).select("link").order("id", desc=True).limit(limit).execute()
            except Exception:
                result = supabase_client.table(table_name).select("link").limit(limit).execute()
        if result.data:
            return set(item['link'] for item in result.data if item.get('link'))
        return set()
    except Exception as e:
        print(f"âŒ Lá»—i khi láº¥y links tá»« DB: {e}")
        return set()

def check_stop_condition(links_to_check, existing_links):
    """Dá»«ng khi 3 bÃ i liÃªn tiáº¿p Ä‘Ã£ cÃ³ trong DB"""
    consecutive_found = 0
    for i, link in enumerate(links_to_check):
        if link in existing_links:
            consecutive_found += 1
            if consecutive_found >= 3:
                return i - 2
        else:
            consecutive_found = 0
    return -1

def insert_article_to_database(db_manager, table_name, article_data, date_parser_func=None):
    """Insert article using new database system"""
    if date_parser_func and article_data.get("date"):
        try:
            parsed_date = date_parser_func(article_data["date"])
            if parsed_date:
                article_data["date"] = parsed_date
        except Exception:
            pass
    return db_manager.insert_article(table_name, article_data)

# ================== FORMAT NGÃ€Y ==================
def convert_date(date_str):
    if not date_str or date_str.strip() == "":
        return None
    formats = [
        "%d-%m-%Y - %I:%M %p",  # 25-07-2025 - 05:52 PM
        "%d-%m-%Y - %H:%M %p",  # 25-07-2025 - 17:52 PM (náº¿u site hiáº¿m gáº·p)
        "%d-%m-%Y",
        "%d/%m/%Y",
    ]
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str.strip(), fmt)
            return format_datetime_for_db(dt)
        except:
            continue
    print(f"âš ï¸ KhÃ´ng parse Ä‘Æ°á»£c ngÃ y: {date_str}")
    return None

# ================== HÃ€M INSERT CHá»NG TRÃ™NG ==================
def insert_to_supabase(db_manager, table_name, data):
    """Wrapper function Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i code cÅ© - sá»­ dá»¥ng hÃ m chung"""
    return insert_article_to_database(db_manager, table_name, data, convert_date)

# ================== HÃ€M SETUP SELENIUM ==================
def setup_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-logging")
    options.add_argument("--log-level=3")
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    options.add_experimental_option('useAutomationExtension', False)
    return webdriver.Chrome(options=options)

# ================== NEW: Helpers láº¥y source_link tá»« CafeF ==================
def _clean_url(u: str) -> Union[str, None]:
    if not u:
        return None
    u = u.strip().strip('"').strip("'").replace("\u00a0", " ").strip()
    if u.startswith("http://") or u.startswith("https://"):
        return u
    return None

def _is_external(u: str) -> bool:
    try:
        host = urlparse(u).netloc.lower()
        return bool(host) and ("cafef.vn" not in host)
    except Exception:
        return False

def extract_source_link_cafef(soup) -> Union[str, None]:
    """
    Láº¥y URL nguá»“n gá»‘c trong bÃ i CafeF:
      1) span.link-source-full (text)
      2) .btn-copy-link-source[data-clipboard-text]
      3) <a href> há»£p lá»‡ trong .link-source-wrapper (loáº¡i javascript:)
    """
    wrapper = soup.select_one("div.link-source-wrapper")
    if not wrapper:
        return None

    full = wrapper.select_one("span.link-source-full")
    if full:
        u = _clean_url(full.get_text(strip=True))
        if u and _is_external(u):
            return u

    btn = wrapper.select_one(".btn-copy-link-source")
    if btn and btn.has_attr("data-clipboard-text"):
        u = _clean_url(btn["data-clipboard-text"])
        if u and _is_external(u):
            return u

    for a in wrapper.select("a[href]"):
        href = (a.get("href") or "").strip()
        if href.lower().startswith("javascript"):
            continue
        u = _clean_url(href)
        if u and _is_external(u):
            return u

    return None
# ===========================================================================

# ================== TRÃCH XUáº¤T Dá»® LIá»†U BÃ€I VIáº¾T ==================
def extract_article_data(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    try:
        title_tag = soup.select_one("h1.title")
        date_tag = soup.select_one("span.pdate[data-role='publishdate']")
        content_container = soup.select_one("div.detail-content.afcbc-body")

        if not (title_tag and date_tag and content_container):
            return None

        content = " ".join(p.get_text(strip=True) for p in content_container.select("p"))

        # ğŸ”— NEW: láº¥y link bÃ i gá»‘c
        source_link = extract_source_link_cafef(soup)

        return {
            "title": title_tag.get_text(strip=True),
            "date": date_tag.get_text(strip=True),
            "content": content,
            "link": driver.current_url,
            "ai_summary": None,           # (náº¿u sau nÃ y cÃ³ thá»ƒ thÃªm)
            "source_link": source_link,   # âœ… THÃŠM VÃ€O DB
        }
    except:
        return None

# ================== CRAWL THEO Tá»ª KHÃ“A Vá»šI Tá»I Æ¯U ==================
def crawl_articles_sequentially(keyword="FPT", table_name="FPT_News", max_pages=1):
    """Crawl articles vá»›i tá»‘i Æ°u thá»i gian"""
    start_time = time.time()
    
    driver = setup_driver()
    wait = WebDriverWait(driver, 10)
    db_manager = get_database_manager()
    
    try:
        existing_links = get_recent_links_from_db(db_manager, table_name, 50)
        all_links = []
        crawled_count = 0
        new_articles = 0
        
        # Láº¥y links tá»« cÃ¡c trang
        for page in range(1, max_pages + 1):
            search_url = f"https://cafef.vn/tim-kiem/trang-{page}.chn?keywords={keyword.replace(' ', '%20')}"
            print(f"ğŸ” PAGE {page}: {search_url}")
            driver.get(search_url)
            time.sleep(2)

            article_links = driver.find_elements(By.CSS_SELECTOR, "div.item h3.titlehidden a")
            page_links = [link.get_attribute("href") for link in article_links if link.get_attribute("href")]
            all_links.extend(page_links)

        print(f"ğŸ“„ Total {len(all_links)} news from {max_pages} pages")

        # ğŸš€ IMPROVED: Kiá»ƒm tra vÃ  lá»c bá» existing links trÆ°á»›c
        links_to_crawl = []
        consecutive_existing = 0
        
        for link in all_links:
            if link in existing_links:
                consecutive_existing += 1
                # Náº¿u 3 bÃ i liÃªn tiáº¿p Ä‘Ã£ cÃ³ â†’ dá»«ng luÃ´n
                if consecutive_existing >= 3:
                    print(f"ğŸ›‘ Found {consecutive_existing} consecutive existing articles - stopping here")
                    break
            else:
                consecutive_existing = 0  # Reset náº¿u gáº·p bÃ i má»›i
                links_to_crawl.append(link)
        
        print(f"ğŸ¯ Crawl {len(links_to_crawl)} bÃ i viáº¿t má»›i" if links_to_crawl else "ğŸ“° No new news")

        # Crawl cÃ¡c bÃ i viáº¿t Ä‘Æ°á»£c chá»n
        duplicate_count = 0
        consecutive_duplicates = 0  # Track liÃªn tiáº¿p duplicate
        
        for i, url in enumerate(links_to_crawl):
            try:
                print(f"ğŸ”— [{i+1}/{len(links_to_crawl)}] {url}")
                driver.get(url)
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "h1.title")))
                time.sleep(1)
                
                data = extract_article_data(driver)
                if data:
                    success = insert_to_supabase(db_manager, table_name, data)
                    if success:
                        new_articles += 1
                        consecutive_duplicates = 0  # Reset khi cÃ³ bÃ i má»›i
                        print(f"âœ… ÄÃ£ lÆ°u bÃ i viáº¿t: {data['title'][:50]}...")
                    else:
                        duplicate_count += 1
                        consecutive_duplicates += 1
                        
                        if duplicate_count <= 3:
                            print(f"âš ï¸  Duplicate title - skipped: {data['title'][:50]}...")
                        elif duplicate_count == 4:
                            print(f"âš ï¸  ... vÃ  {len(links_to_crawl) - i - 1} duplicates khÃ¡c (áº©n bá»›t log)")
                        
                        # ğŸš€ EARLY STOP: Náº¿u 5 bÃ i liÃªn tiáº¿p duplicate â†’ dá»«ng ngay
                        if consecutive_duplicates >= 5:
                            print(f"ğŸ›‘ Stopped crawling - Found {consecutive_duplicates} consecutive duplicates")
                            print(f"ğŸ“Š Saved {new_articles} new articles, skipped {duplicate_count} duplicates")
                            break
                else:
                    print("âš ï¸  KhÃ´ng láº¥y Ä‘Æ°á»£c dá»¯ liá»‡u bÃ i viáº¿t")
                
                crawled_count += 1
                time.sleep(1)
            except Exception as e:
                print(f"âŒ Lá»—i khi crawl bÃ i viáº¿t {i+1}: {e}")
                continue

    except Exception as e:
        print(f"âŒ Lá»—i chung cho keyword {keyword}: {e}")
    finally:
        driver.quit()
        db_manager.close_connections()
        
        duration = time.time() - start_time
        return {
            'keyword': keyword,
            'table_name': table_name,
            'duration': duration,
            'total_found': len(all_links),
            'crawled_count': crawled_count,
            'new_articles': new_articles,
            'stopped_early': consecutive_existing >= 3
        }

# ================== MAIN Vá»šI DASHBOARD ==================
def main_cafef():
    start_time = time.time()
    start_time_str = datetime.now().strftime("%H:%M:%S")
    
    print("\n" + "â•" * 60)
    print("ğŸš€ CAFEF KEYWORD CRAWLER DASHBOARD".center(60))
    print(f"â° Started: {start_time_str}".center(60))
    print("â•" * 60)

    keyword_table_map = {
        "FPT": "FPT_News",
        "GAS": "GAS_News", 
        "IMP": "IMP_News",
        "VCB": "VCB_News",
    }
    
    results = []
    for kw, table_name in keyword_table_map.items():
        print(f"\nğŸš€ Processing keyword CAFEF: ---{kw}---  â†’ Save to {table_name}")
        result = crawl_articles_sequentially(keyword=kw, table_name=table_name, max_pages=1)
        results.append(result)
        if kw != list(keyword_table_map.keys())[-1]:
            time.sleep(3)
    
    end_time = time.time()
    total_duration = end_time - start_time
    total_found = sum(r['total_found'] for r in results)
    total_crawled = sum(r['crawled_count'] for r in results)
    total_new = sum(r['new_articles'] for r in results)
    
    print("\n" + "â•" * 60)
    print("ğŸ‰ CRAWLING CAFEF KEYWORD CRAWLER COMPLETED - RESULTS".center(60))
    print("â•" * 60)
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Keyword â”‚ Time   â”‚ Status       â”‚ Saved Articles    â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    for result in results:
        keyword = result['keyword']
        duration = result['duration']
        new_count = result['new_articles']
        status = "No new news" if new_count == 0 else "New news"
        results_text = f"{new_count} saved"
        print(f"â”‚ {keyword:<7} â”‚ {duration:>6.1f}s â”‚ {status:<12} â”‚ {results_text:<17} â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("\n" + "â•" * 60)
    print("ğŸ“Š SUMMARY CAFEF KEYWORD CRAWLER".center(60))
    print("â”€" * 60)
    print(f"â±ï¸  Total Time      : {total_duration:.1f}s ({total_duration/60:.1f} minutes)")
    print(f"ğŸ“Š Total Found     : {total_found} articles")
    print(f"âœ… Total New       : {total_new} articles")
    print(f"âš¡ Avg per Keyword : {total_duration/len(results):.1f}s")
    print("â•" * 60)
    print("ğŸ¯ CAFEF KEYWORD CRAWLING COMPLETED!")
    print("â•" * 60)

if __name__ == "__main__":
    main_cafef()
