from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import requests
import re
from datetime import datetime
import warnings
import logging

# ğŸ”‡ Táº¯t hoÃ n toÃ n cÃ¡c warning vÃ  log khÃ´ng cáº§n thiáº¿t
warnings.filterwarnings("ignore")
logging.getLogger('urllib3').setLevel(logging.ERROR)
logging.getLogger('selenium').setLevel(logging.ERROR)

# Import centralized database system
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from database import SupabaseManager, DatabaseConfig, format_datetime_for_db

# Constants
STOCK_CODES = ["FPT", "GAS", "IMP", "VCB"]

# Helper functions
def get_database_manager():
    """Get database manager instance"""
    return SupabaseManager()

def get_table_name(stock_code=None, is_general=False):
    """Get table name using new config"""
    config = DatabaseConfig()
    return config.get_table_name(stock_code=stock_code, is_general=is_general)

def get_recent_links_from_db(db_manager, table_name, limit=50):
    """Láº¥y 50 link bÃ i viáº¿t gáº§n nháº¥t tá»« database cho chungta crawling"""
    try:
        supabase_client = db_manager.get_supabase_client()
        # Thá»­ dÃ¹ng id thay vÃ¬ created_at náº¿u khÃ´ng cÃ³ created_at
        try:
            result = supabase_client.table(table_name).select("link").order("created_at", desc=True).limit(limit).execute()
        except Exception:
            # Fallback: sá»­ dá»¥ng id hoáº·c khÃ´ng order
            try:
                result = supabase_client.table(table_name).select("link").order("id", desc=True).limit(limit).execute()
            except Exception:
                # Fallback cuá»‘i: chá»‰ láº¥y link khÃ´ng order
                result = supabase_client.table(table_name).select("link").limit(limit).execute()
        
        if result.data:
            return set(item['link'] for item in result.data if item.get('link'))
        return set()
    except Exception as e:
        print(f"âŒ Lá»—i khi láº¥y links tá»« DB: {e}")
        return set()

def check_stop_condition(links_to_check, existing_links):
    """
    Kiá»ƒm tra Ä‘iá»u kiá»‡n dá»«ng: 3 bÃ i liÃªn tiáº¿p cÃ³ trong DB
    Args:
        links_to_check: List cÃ¡c link cáº§n kiá»ƒm tra (theo thá»© tá»± tá»« trÃªn xuá»‘ng)
        existing_links: Set cÃ¡c link Ä‘Ã£ cÃ³ trong DB
    Returns:
        int: Index Ä‘á»ƒ dá»«ng (náº¿u tÃ¬m tháº¥y 3 bÃ i liÃªn tiáº¿p), -1 náº¿u khÃ´ng
    """
    consecutive_found = 0
    
    for i, link in enumerate(links_to_check):
        if link in existing_links:
            consecutive_found += 1
            if consecutive_found >= 3:
                # Dá»«ng táº¡i vá»‹ trÃ­ bÃ i thá»© 3 liÃªn tiáº¿p
                return i - 2  # Tráº£ vá» index cá»§a bÃ i Ä‘áº§u tiÃªn trong 3 bÃ i liÃªn tiáº¿p
        else:
            consecutive_found = 0  # Reset náº¿u khÃ´ng liÃªn tiáº¿p
    
    return -1  # KhÃ´ng tÃ¬m tháº¥y 3 bÃ i liÃªn tiáº¿p

def insert_article_to_database(db_manager, table_name, article_data, date_parser_func=None):
    """Insert article using new database system"""
    # Parse date if parser provided
    if date_parser_func and article_data.get("date"):
        try:
            parsed_date = date_parser_func(article_data["date"])
            if parsed_date:
                article_data["date"] = parsed_date
        except Exception:
            pass
    
    return db_manager.insert_article(table_name, article_data)

def normalize_date_only(raw_text):
    if not raw_text or raw_text.strip() == "" or raw_text.strip().upper() == "EMPTY":
        return None

    raw_text = raw_text.strip()

    # TrÆ°á»ng há»£p dáº¡ng 23-07-2025 - 06:57 AM
    try:
        dt = datetime.strptime(raw_text, "%d-%m-%Y - %I:%M %p")
        return format_datetime_for_db(dt)
    except:
        pass

    # TrÆ°á»ng há»£p dáº¡ng Thá»© sÃ¡u, 25/7/2025 | 18:08GMT hoáº·c 30/7/2025
    try:
        match = re.search(r"(\d{1,2})/(\d{1,2})/(\d{4})", raw_text)
        if match:
            day, month, year = match.groups()
            dt = datetime(int(year), int(month), int(day))
            return format_datetime_for_db(dt)
    except:
        pass
    return None

# ğŸ”¹ Crawl dá»¯ liá»‡u tá»« Chungta.vn vá»›i tá»‘i Æ°u
def crawl_chungta(url, table_name, db_manager):
    """Crawl Chungta.vn vá»›i logic tá»‘i Æ°u"""
    start_time = time.time()
    
    existing_links = get_recent_links_from_db(db_manager, table_name, 100)  #100 news gáº§n nháº¥t

    options = Options()
    options.add_argument("--headless")  # Cháº¡y áº©n Ä‘á»ƒ trÃ¡nh bá»‹ interrupt
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    # Táº¯t logs
    options.add_argument("--disable-logging")
    options.add_argument("--log-level=3")
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    options.add_experimental_option('useAutomationExtension', False)
    driver = webdriver.Chrome(options=options)

    driver.get(url)
    wait = WebDriverWait(driver, 10)

    MAX_PAGE = 1
    print(f"ğŸ”„ LOAD PAGE {MAX_PAGE}...")

    for i in range(MAX_PAGE):
        try:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            button = wait.until(EC.element_to_be_clickable((By.ID, "load_more_redesign")))
            button.click()
            time.sleep(4)
        except:
            print(f"  âš ï¸ KhÃ´ng thá»ƒ load thÃªm trang (dá»«ng táº¡i page {i})")
            break

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    articles = soup.select("h3.title-news a")
    print(f"ğŸ“„ Total {len(articles)} news from {MAX_PAGE} pages")
    # Táº¡o list cÃ¡c links Ä‘á»ƒ check
    all_links = []
    for a in articles:
        link = "https://chungta.vn" + a.get("href")
        all_links.append(link)
    
    # Kiá»ƒm tra Ä‘iá»u kiá»‡n dá»«ng (3 bÃ i liÃªn tiáº¿p cÃ³ trong DB)
    stop_index = check_stop_condition(all_links, existing_links)
    
    if stop_index >= 0:
        links_to_crawl = all_links[:stop_index]
        if len(links_to_crawl) > 0:
            print(f"ğŸ¯ Crawl {len(links_to_crawl)} bÃ i viáº¿t má»›i")
        else:
            print(f"ğŸ“° No new news")
    else:
        # Lá»c chá»‰ nhá»¯ng link chÆ°a cÃ³ trong DB
        links_to_crawl = [link for link in all_links if link not in existing_links]
        if len(links_to_crawl) > 0:
            print(f"ğŸ¯ Crawl {len(links_to_crawl)} bÃ i viáº¿t má»›i")
        else:
            print(f"ğŸ“° No new news")

    # Crawl cÃ¡c bÃ i viáº¿t Ä‘Æ°á»£c chá»n
    new_articles = 0
    crawled_count = 0
    duplicate_count = 0
    headers = {"User-Agent": "Mozilla/5.0"}

    for i, link in enumerate(links_to_crawl):
        try:
            print(f"ğŸ”— [{i+1}/{len(links_to_crawl)}] {link}")
            
            res = requests.get(link, headers=headers, timeout=10)
            article_soup = BeautifulSoup(res.text, "html.parser")

            # Láº¥y title tá»« link ban Ä‘áº§u
            title_element = None
            for a in articles:
                if "https://chungta.vn" + a.get("href") == link:
                    title_element = a
                    break
            
            title_preview = title_element.get_text(strip=True) if title_element else ""
            
            title = article_soup.select_one("h1.title-detail")
            title = title.get_text(strip=True) if title else title_preview

            date = article_soup.select_one("span.time")
            date = date.get_text(strip=True) if date else "KhÃ´ng rÃµ ngÃ y"

            content = article_soup.select_one("article.fck_detail.width_common")
            content = content.get_text(separator="\n", strip=True) if content else ""

            article_data = {
                "title": title,
                "date": date,
                "link": link,
                "content": content,
                "ai_summary": "" 
            }
            
            success = insert_article_to_database(db_manager, table_name, article_data, normalize_date_only)
            if success:
                new_articles += 1
                print(f"âœ… ÄÃ£ lÆ°u bÃ i viáº¿t: {title[:50]}...")
            else:
                duplicate_count += 1
                # Chá»‰ hiá»ƒn thá»‹ 3 duplicate Ä‘áº§u tiÃªn Ä‘á»ƒ trÃ¡nh spam
                if duplicate_count <= 3:
                    print(f"âš ï¸  Duplicate title - skipped: {title[:50]}...")
                elif duplicate_count == 4:
                    print(f"âš ï¸  ... vÃ  {len(links_to_crawl) - i - 1} duplicates khÃ¡c (khÃ´ng hiá»ƒn thá»‹)")
            
            crawled_count += 1
            time.sleep(1)  # Delay between requests

        except Exception as e:
            print(f"âŒ Lá»—i láº¥y bÃ i {link}: {e}")
            continue

    # TÃ­nh toÃ¡n káº¿t quáº£
    end_time = time.time()
    duration = end_time - start_time
    
    return {
        'url': url,
        'duration': duration,
        'total_found': len(articles),
        'crawled_count': crawled_count,
        'new_articles': new_articles,
        'stopped_early': stop_index >= 0
    }

def main_chungta():
    """Main function vá»›i dashboard timing vÃ  thá»‘ng kÃª"""
    start_time = time.time()
    start_time_str = datetime.now().strftime("%H:%M:%S")
    
    print("\n" + "â•" * 60)
    print("ğŸš€ CHUNGTA CRAWLER DASHBOARD".center(60))
    print(f"â° Started: {start_time_str}".center(60))
    print("â•" * 60)

    urls = [
        "https://chungta.vn/kinh-doanh",
        "https://chungta.vn/cong-nghe"
    ]
    table_name = "FPT_News"  # Chung Ta lÆ°u vÃ o FPT_News vÃ¬ cÃ³ nhiá»u tin vá» FPT
    db_manager = get_database_manager()

    results = []
    
    for i, url in enumerate(urls, 1):
        print(f"\nğŸš€ â•â•â•â•â•â•â•â•â• Processing URL [{i}/{len(urls)}]: {url} â•â•â•â•â•â•â•â•â•")
        print(f"ğŸ“‹ Save to ====>>>  {table_name}")
        
        result = crawl_chungta(url, table_name, db_manager)
        results.append(result)
        
        # Delay between URLs
        if i < len(urls):
            time.sleep(3)

    db_manager.close_connections()
    
    # Hiá»ƒn thá»‹ dashboard káº¿t quáº£
    end_time = time.time()
    total_duration = end_time - start_time
    total_found = sum(r['total_found'] for r in results)
    total_crawled = sum(r['crawled_count'] for r in results)
    total_new = sum(r['new_articles'] for r in results)
    
    print("\n" + "â•" * 60)
    print("ğŸ‰ CRAWLING COMPLETED - RESULTS".center(60))
    print("â•" * 60)
    
    # Table header
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ URL                      â”‚ Time   â”‚ Status       â”‚ Saved Articles    â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    # Table rows
    for result in results:
        url_short = result['url'].replace("https://chungta.vn/", "").ljust(24)[:24]
        duration = result['duration']
        new_count = result['new_articles']
        stopped = result['stopped_early']
        
        # Status Ä‘Æ¡n giáº£n
        status = "No new news" if new_count == 0 else "New news"
        # Saved Articles = sá»‘ bÃ i thá»±c sá»± Ä‘Æ°á»£c lÆ°u
        results_text = f"{new_count} saved"
        
        print(f"â”‚ {url_short} â”‚ {duration:>6.1f}s â”‚ {status:<12} â”‚ {results_text:<17} â”‚")
    
    # Table footer
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # Summary
    print("\n" + "â•" * 60)
    print("ğŸ“Š SUMMARY".center(60))
    print("â”€" * 60)
    print(f"â±ï¸  Total Time      : {total_duration:.1f}s ({total_duration/60:.1f} minutes)")
    print(f"ğŸ“Š Total Found     : {total_found} articles")
    print(f"âœ… Total New       : {total_new} articles")
    print(f"âš¡ Avg per URL     : {total_duration/len(results):.1f}s")
    print("â•" * 60)
    print("ğŸ¯ CHUNGTA CRAWLING COMPLETED!")
    print("â•" * 60)

if __name__ == "__main__":
    main_chungta()