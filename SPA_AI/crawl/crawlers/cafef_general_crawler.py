from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
from datetime import datetime
import warnings
import logging
from urllib.parse import urlparse   # ‚úÖ NEW
from typing import Union

# üîá T·∫Øt ho√†n to√†n c√°c warning v√† log kh√¥ng c·∫ßn thi·∫øt
warnings.filterwarnings("ignore")
logging.getLogger('urllib3').setLevel(logging.ERROR)
logging.getLogger('selenium').setLevel(logging.ERROR)

# Import centralized database system
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from database import SupabaseManager, DatabaseConfig, format_datetime_for_db

# Constants
STOCK_CODES = ["FPT", "GAS", "IMP", "VCB"]

# Helper functions
def get_database_manager():
    """Get database manager instance"""
    return SupabaseManager()

def get_table_name(stock_code=None, is_general=False):
    """Get table name using new config"""
    config = DatabaseConfig()
    return config.get_table_name(stock_code=stock_code, is_general=is_general)

def get_recent_links_from_db(db_manager, table_name="General_News", limit=100):
    """L·∫•y 100 link b√†i vi·∫øt g·∫ßn nh·∫•t t·ª´ database"""
    try:
        supabase_client = db_manager.get_supabase_client()
        # Th·ª≠ d√πng id thay v√¨ created_at n·∫øu kh√¥ng c√≥ created_at
        try:
            result = supabase_client.table(table_name).select("link").order("created_at", desc=True).limit(limit).execute()
        except Exception:
            # Fallback: s·ª≠ d·ª•ng id ho·∫∑c kh√¥ng order
            try:
                result = supabase_client.table(table_name).select("link").order("id", desc=True).limit(limit).execute()
            except Exception:
                # Fallback cu·ªëi: ch·ªâ l·∫•y link kh√¥ng order
                result = supabase_client.table(table_name).select("link").limit(limit).execute()
        
        if result.data:
            return set(item['link'] for item in result.data if item.get('link'))
        return set()
    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y links t·ª´ DB: {e}")
        return set()

def check_stop_condition(links_to_check, existing_links):
    """
    Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng: 5 b√†i li√™n ti·∫øp c√≥ trong DB
    """
    consecutive_found = 0
    for i, link in enumerate(links_to_check):
        if link in existing_links:
            consecutive_found += 1
            if consecutive_found >= 5:
                return i - 4  # index c·ªßa b√†i ƒë·∫ßu ti√™n trong 5 b√†i li√™n ti·∫øp
        else:
            consecutive_found = 0
    return -1

def insert_article_to_database(db_manager, table_name, article_data, date_parser_func=None):
    """Insert article using new database system"""
    if date_parser_func and article_data.get("date"):
        try:
            parsed_date = date_parser_func(article_data["date"])
            if parsed_date:
                article_data["date"] = parsed_date
        except Exception:
            pass
    return db_manager.insert_article(table_name, article_data)

def convert_date(date_str):
    formats = ["%d-%m-%Y - %H:%M %p", "%d-%m-%Y", "%d/%m/%Y"]
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str.strip(), fmt)
            return format_datetime_for_db(dt)
        except:
            pass
    return None

def insert_to_supabase(db_manager, table_name, data):
    """Wrapper function ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code c≈© - s·ª≠ d·ª•ng h√†m chung"""
    return insert_article_to_database(db_manager, table_name, data, convert_date)

def setup_driver():
    options = Options()
    options.add_argument("--headless")  # Ch·∫°y ·∫©n ƒë·ªÉ tr√°nh b·ªã interrupt
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    # T·∫Øt logs
    options.add_argument("--disable-logging")
    options.add_argument("--log-level=3")
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    options.add_experimental_option('useAutomationExtension', False)
    return webdriver.Chrome(options=options)

# ========= NEW: Helpers cho source_link (CafeF) =========
def _clean_url(u: str) -> Union[str, None]:
    if not u:
        return None
    u = u.strip().strip('"').strip("'")
    u = u.replace("\u00a0", " ").strip()  # b·ªè NBSP
    if u.startswith("http://") or u.startswith("https://"):
        return u
    return None

def _is_external(u: str) -> bool:
    try:
        host = urlparse(u).netloc.lower()
        return bool(host) and ("cafef.vn" not in host)
    except Exception:
        return False

def extract_source_link_cafef(soup) -> Union[str, None]:
    """
    Tr·∫£ v·ªÅ URL g·ªëc (ngu·ªìn) trong trang b√†i vi·∫øt CafeF, ho·∫∑c None n·∫øu kh√¥ng c√≥.
    ∆Øu ti√™n:
      1) span.link-source-full (text ch·ª©a URL)
      2) .btn-copy-link-source[data-clipboard-text]
      3) <a> h·ª£p l·ªá trong .link-source-wrapper (kh√¥ng ph·∫£i javascript:)
    """
    wrapper = soup.select_one("div.link-source-wrapper")
    if not wrapper:
        return None

    # 1) URL ·ªü d·∫°ng text
    full = wrapper.select_one("span.link-source-full")
    if full:
        u = _clean_url(full.get_text(strip=True))
        if u and _is_external(u):
            return u

    # 2) N√∫t copy c√≥ data-clipboard-text
    btn = wrapper.select_one(".btn-copy-link-source")
    if btn and btn.has_attr("data-clipboard-text"):
        u = _clean_url(btn["data-clipboard-text"])
        if u and _is_external(u):
            return u

    # 3) Fallback: href trong <a>
    for a in wrapper.select("a[href]"):
        href = (a.get("href") or "").strip()
        if href.lower().startswith("javascript"):
            continue
        u = _clean_url(href)
        if u and _is_external(u):
            return u

    return None
# =======================================================

def extract_article_data(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    title = soup.select_one("h1.title")
    date_tag = soup.select_one("span.pdate[data-role='publishdate']")
    content_tag = soup.select_one("div.detail-content.afcbc-body")

    if not (title and date_tag and content_tag):
        return None

    content = " ".join(p.get_text(strip=True) for p in content_tag.select("p"))

    # üîó NEW: l·∫•y link b√†i g·ªëc t·ª´ CafeF
    source_link = extract_source_link_cafef(soup)

    return {
        "title": title.get_text(strip=True),
        "date": date_tag.get_text(strip=True),
        "content": content,
        "link": driver.current_url,   # link b√†i tr√™n CafeF
        "ai_summary": None,
        "source_link": source_link    # ‚úÖ TH√äM V√ÄO DB
    }

def click_view_more(driver, max_clicks=5):
    for i in range(max_clicks):
        try:
            driver.execute_script("window.scrollBy(0, 1200);")
            time.sleep(1)
            btn = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "div.btn-viewmore"))
            )
            driver.execute_script("arguments[0].scrollIntoView();", btn)
            time.sleep(0.5)
            btn.click()
            print(f"üîΩ Click Xem th√™m ({i+1}/{max_clicks})")
            time.sleep(2)
        except:
            break

def crawl_cafef_chung(max_clicks=5):
    """Crawl tin t·ª©c CafeF v·ªõi t·ªëi ∆∞u th·ªùi gian"""
    start_time = time.time()
    start_time_str = datetime.now().strftime("%H:%M:%S")
    
    print("\n" + "‚ïê" * 60)
    print("üöÄ CAFEF GENERAL NEWS CRAWLER".center(60))
    print(f"‚è∞ Started: {start_time_str}".center(60))
    print("‚ïê" * 60)
    
    driver = setup_driver()
    db_manager = get_database_manager()
    
    try:
        driver.get("https://cafef.vn/thi-truong-chung-khoan.chn")
        time.sleep(3)

        # L·∫•y danh s√°ch links ƒë√£ c√≥ trong DB (100 b√†i g·∫ßn nh·∫•t)
        print("üîç Checking database for crawling optimization...")
        existing_links = get_recent_links_from_db(db_manager, "General_News", 100)  #100 news 

        # Click "Xem th√™m" ƒë·ªÉ load th√™m b√†i vi·∫øt
        print(f"üîÑ Click 'load more' {max_clicks} l·∫ßn...")
        click_view_more(driver, max_clicks=max_clicks)

        # L·∫•y t·∫•t c·∫£ links b√†i vi·∫øt
        link_elements = driver.find_elements(By.CSS_SELECTOR, "div.tlitem.box-category-item h3 a")
        all_links = []
        
        for link_el in link_elements:
            url = link_el.get_attribute("href")
            if url:
                all_links.append(url)
                
        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng (5 b√†i li√™n ti·∫øp c√≥ trong DB)
        stop_index = check_stop_condition(all_links, existing_links)
        
        if stop_index >= 0:
            links_to_crawl = all_links[:stop_index]
            print(f"üéØ Crawl {len(links_to_crawl)} new news")
        else:
            links_to_crawl = all_links
            print(f"üéØ Crawl all {len(links_to_crawl)} new news")

        # Crawl c√°c b√†i vi·∫øt ƒë∆∞·ª£c ch·ªçn
        crawled_count = 0
        new_articles = 0
        
        for i, url in enumerate(links_to_crawl):
            try:
                print(f"üîó [{i+1}/{len(links_to_crawl)}] {url}")
                
                # Skip n·∫øu link ƒë√£ c√≥ trong DB
                if url in existing_links:
                    print(f"‚è≠Ô∏è  B·ªè qua - ƒë√£ c√≥ trong DB")
                    continue
                
                driver.execute_script("window.open(arguments[0]);", url)
                driver.switch_to.window(driver.window_handles[-1])
                time.sleep(1)
                
                data = extract_article_data(driver)
                if data:
                    # DEBUG n·∫øu c·∫ßn: print("DEBUG source_link =", data.get("source_link"))
                    success = insert_to_supabase(db_manager, "General_News", data)
                    if success:
                        new_articles += 1
                        print(f"‚úÖ ƒê√£ l∆∞u b√†i vi·∫øt: {data['title'][:50]}...")
                    else:
                        print(f"‚è≠Ô∏è  B·ªè qua - c√≥ th·ªÉ ƒë√£ t·ªìn t·∫°i: {data['title'][:50]}...")
                else:
                    print(f"‚ö†Ô∏è  Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu b√†i vi·∫øt")
                
                crawled_count += 1
                
                driver.close()
                driver.switch_to.window(driver.window_handles[0])
                time.sleep(1)
                
            except Exception as e:
                print(f"‚ùå L·ªói khi crawl b√†i vi·∫øt {i+1}: {e}")
                continue

    except Exception as e:
        print(f"‚ùå L·ªói chung: {e}")
    finally:
        driver.quit()
        db_manager.close_connections()
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        end_time = time.time()
        duration = end_time - start_time
        
        print("\n" + "‚ïê" * 60)
        print("üéâ CRAWLING CAFEF GENERAL COMPLETED - RESULTS".center(60))
        print("‚ïê" * 60)
        
        print(f"‚è±Ô∏è  Total Time     : {duration:.1f}s ({duration/60:.1f} minutes)")
        print(f"üìä Articles Found : {len(all_links)} total")
        print(f"üéØ Articles Crawled: {crawled_count}")
        print(f"‚úÖ New Articles   : {new_articles}")
        print(f"‚è≠Ô∏è  Skipped (Exists): {len(links_to_crawl) - crawled_count}")
        print(f"‚ö° Avg per Article: {duration/max(crawled_count, 1):.1f}s")
        
        if stop_index >= 0:
            print("üõë Stopped early due to no new posts.")

        print("‚ïê" * 60)
        print("üéâ CAFEF GENERAL CRAWLING COMPLETED SUCCESSFULLY!")
        print("‚ïê" * 60)

if __name__ == "__main__":
    crawl_cafef_chung(max_clicks=2)
