from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime, timedelta
from dateutil import parser
import time
import re
import warnings
import logging
from urllib.parse import urljoin, urlparse  # âœ… NEW

# ğŸ”‡ Táº¯t hoÃ n toÃ n cÃ¡c warning vÃ  log khÃ´ng cáº§n thiáº¿t
warnings.filterwarnings("ignore")
logging.getLogger('urllib3').setLevel(logging.ERROR)
logging.getLogger('selenium').setLevel(logging.ERROR)

# Import centralized database system
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from database import SupabaseManager, DatabaseConfig, format_datetime_for_db

# Constants from old config
FIREANT_BASE_URL = "https://fireant.vn"
FIREANT_STOCK_URL = "https://fireant.vn/ma-chung-khoan"
FIREANT_ARTICLE_URL = "https://fireant.vn/bai-viet"
STOCK_CODES = ["FPT", "GAS", "IMP", "VCB"]

# Helper functions to replace old config functions
def get_recent_links_from_db(db_manager, table_name, limit=100):
    """Láº¥y 100 link bÃ i viáº¿t gáº§n nháº¥t tá»« database Ä‘á»ƒ tá»‘i Æ°u crawling"""
    try:
        supabase_client = db_manager.get_supabase_client()
        # Thá»­ dÃ¹ng created_at trÆ°á»›c, fallback sang id
        try:
            result = supabase_client.table(table_name).select("link").order("created_at", desc=True).limit(limit).execute()
        except Exception:
            # Fallback: sá»­ dá»¥ng id hoáº·c khÃ´ng order
            try:
                result = supabase_client.table(table_name).select("link").order("id", desc=True).limit(limit).execute()
            except Exception:
                # Fallback cuá»‘i: chá»‰ láº¥y link khÃ´ng order
                result = supabase_client.table(table_name).select("link").limit(limit).execute()
        
        if result.data:
            return set(item['link'] for item in result.data if item.get('link'))
        return set()
    except Exception as e:
        print(f"âŒ Lá»—i khi láº¥y links tá»« DB: {e}")
        return set()

def check_stop_condition(links_to_check, existing_links):
    """
    Kiá»ƒm tra Ä‘iá»u kiá»‡n dá»«ng: 3 bÃ i liÃªn tiáº¿p cÃ³ trong DB
    Args:
        links_to_check: List cÃ¡c link cáº§n kiá»ƒm tra (theo thá»© tá»± tá»« trÃªn xuá»‘ng)
        existing_links: Set cÃ¡c link Ä‘Ã£ cÃ³ trong DB
    Returns:
        int: Index Ä‘á»ƒ dá»«ng (náº¿u tÃ¬m tháº¥y 3 bÃ i liÃªn tiáº¿p), -1 náº¿u khÃ´ng
    """
    consecutive_found = 0
    
    for i, link in enumerate(links_to_check):
        if link in existing_links:
            consecutive_found += 1
            if consecutive_found >= 3:
                # Dá»«ng táº¡i vá»‹ trÃ­ bÃ i thá»© 3 liÃªn tiáº¿p
                return i - 2  # Tráº£ vá» index cá»§a bÃ i Ä‘áº§u tiÃªn trong 3 bÃ i liÃªn tiáº¿p
        else:
            consecutive_found = 0  # Reset náº¿u khÃ´ng liÃªn tiáº¿p
    
    return -1  # KhÃ´ng tÃ¬m tháº¥y 3 bÃ i liÃªn tiáº¿p

def get_database_manager():
    """Get database manager instance"""
    return SupabaseManager()

def get_table_name(stock_code=None, is_general=False):
    """Get table name using new config"""
    config = DatabaseConfig()
    return config.get_table_name(stock_code=stock_code, is_general=is_general)

def get_stock_url(stock_code):
    """Generate stock URL"""
    return f"{FIREANT_STOCK_URL}/{stock_code}"

def insert_article_to_database(db_manager, table_name, article_data, date_parser_func=None):
    """Insert article using new database system"""
    # Parse date if parser provided
    if date_parser_func and article_data.get("date"):
        try:
            parsed_date = date_parser_func(article_data["date"])
            if parsed_date:
                article_data["date"] = parsed_date
        except Exception:
            pass
    
    return db_manager.insert_article(table_name, article_data)

# Configuration constants
MAX_SCROLLS = 5  # Sá»‘ láº§n scroll 

def parse_fuzzy_datetime(raw_text, current_year):
    if not raw_text:
        return None
        
    raw_text = raw_text.strip()
    original_text = raw_text
    raw_text = raw_text.lower()
    
    try:
        if "hÃ´m nay" in raw_text:
            time_part = raw_text.replace("hÃ´m nay", "").strip()
            dt = datetime.strptime(time_part, "%H:%M")
            today = datetime.now()
            return today.replace(hour=dt.hour, minute=dt.minute, second=0, microsecond=0)

        if "hÃ´m qua" in raw_text:
            time_part = raw_text.replace("hÃ´m qua", "").strip()
            dt = datetime.strptime(time_part, "%H:%M")
            yesterday = datetime.now() - timedelta(days=1)
            return yesterday.replace(hour=dt.hour, minute=dt.minute, second=0, microsecond=0)

        # Xá»­ lÃ½ "20 phÃºt", "22 phÃºt", "30 phÃºt" trÆ°á»›c
        match = re.match(r"(\d+)\s*phÃºt", raw_text)
        if match:
            minutes_ago = int(match.group(1))
            return datetime.now() - timedelta(minutes=minutes_ago)

        elif "khoáº£ng" in raw_text or "trÆ°á»›c" in raw_text:
            return None 

        # Danh sÃ¡ch cÃ¡c format ngÃ y cÃ³ thá»ƒ
        date_formats = [
            "%Y-%m-%d",           # 2025-08-02
            "%d/%m/%Y",           # 02/08/2025  
            "%d-%m-%Y",           # 02-08-2025
            "%d/%m %H:%M",        # 02/08 14:30
            "%d-%m-%Y %H:%M",     # 02-08-2025 14:30
            "%Y-%m-%d %H:%M:%S",  # 2025-08-02 14:30:00
        ]
        
        for fmt in date_formats:
            try:
                dt = datetime.strptime(original_text, fmt)
                # Náº¿u khÃ´ng cÃ³ nÄƒm thÃ¬ dÃ¹ng current_year
                if "%Y" not in fmt:
                    dt = dt.replace(year=current_year)
                return dt
            except:
                continue
                
        # Náº¿u khÃ´ng match format nÃ o thÃ¬ return None
        return None

    except Exception as e:
        print(f"âš ï¸ Lá»—i parse fuzzy time: '{original_text}' ({e})")
        return None

# def format_datetime_obj(dt):
#     return dt.strftime("%d-%m-%Y - %I:%M %p")

# def format_datetime_obj(dt):
#     day = dt.day
#     month = dt.month
#     year = dt.year
#     return f"{day}/{month}/{year}"

def format_datetime_obj(dt):
    return format_datetime_for_db(dt)

def fireant_date_parser(raw_text, current_year=2025):
    """Date parser cho FireAnt"""
    return parse_fuzzy_datetime(raw_text, current_year)


def setup_driver():
    options = Options()
    options.add_argument("--headless")  
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-logging")
    options.add_argument("--log-level=3")
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    options.add_experimental_option('useAutomationExtension', False)
    return webdriver.Chrome(options=options)

# ============================================
# ğŸ”— NEW: Láº¥y link gá»‘c trong #post_content
# ============================================
def extract_source_link_from_post(soup):
    """TÃ¬m link gá»‘c trong pháº§n #post_content cá»§a bÃ i viáº¿t FireAnt."""
    content_div = soup.find("div", id="post_content")
    if not content_div:
        return None

    anchors = content_div.select("a[href]")
    if not anchors:
        return None

    def normalize_href(href: str) -> str:
        href = (href or "").strip()
        if not href:
            return ""
        return urljoin(FIREANT_BASE_URL, href) if href.startswith("/") else href

    def is_external(href: str) -> bool:
        try:
            u = urlparse(href)
            return bool(u.netloc) and ("fireant.vn" not in u.netloc.lower())
        except Exception:
            return False

    KEYWORDS = ("Link gá»‘c", "nguá»“n", "source", "xem thÃªm", "Ä‘á»c thÃªm",)

    # Æ¯u tiÃªn cÃ¡c anchor cÃ³ text phÃ¹ há»£p, duyá»‡t tá»« dÆ°á»›i lÃªn (cuá»‘i bÃ i trÆ°á»›c)
    for a in reversed(anchors):
        text = (a.get_text(strip=True) or "").lower()
        href = normalize_href(a.get("href", ""))
        if any(k in text for k in KEYWORDS) and is_external(href):
            return href

    # Fallback: láº¥y external link cuá»‘i cÃ¹ng trong pháº§n ná»™i dung
    for a in reversed(anchors):
        href = normalize_href(a.get("href", ""))
        if is_external(href):
            return href

    return None

def scroll_and_collect_links(driver, stock_code="FPT", scroll_step=500):
    url = get_stock_url(stock_code)
    driver.get(url)
    time.sleep(5)

    # Thu tháº­p links theo thá»© tá»± xuáº¥t hiá»‡n tá»« trÃªn xuá»‘ng
    links = []
    seen_links = set()  # Äá»ƒ track cÃ¡c link Ä‘Ã£ tháº¥y
    scroll_position = 0

    # Äáº§u tiÃªn thu tháº­p links á»Ÿ top cá»§a trang
    initial_articles = driver.find_elements(By.CSS_SELECTOR, "div.flex.flex-row.h-full.border-b-1 a[href^='/bai-viet/']")
    for article in initial_articles:
        try:
            href = article.get_attribute("href")
            if href and href.startswith("https://fireant.vn/bai-viet/"):
                clean_href = href.split("?")[0]
                if clean_href not in seen_links:
                    links.append(clean_href)
                    seen_links.add(clean_href)
        except:
            continue

    for i in range(MAX_SCROLLS):
        print(f"ğŸ”½ Scroll {i+1}/{MAX_SCROLLS}")
        scroll_position += scroll_step
        driver.execute_script(f"window.scrollTo(0, {scroll_position});")
        time.sleep(4)

        # Chá»‰ thu tháº­p cÃ¡c link Má»šI xuáº¥t hiá»‡n sau khi scroll
        current_articles = driver.find_elements(By.CSS_SELECTOR, "div.flex.flex-row.h-full.border-b-1 a[href^='/bai-viet/']")
        new_links_found = 0
        
        for article in current_articles:
            try:
                href = article.get_attribute("href")
                if href and href.startswith("https://fireant.vn/bai-viet/"):
                    clean_href = href.split("?")[0]
                    if clean_href not in seen_links:
                        links.append(clean_href)
                        seen_links.add(clean_href)
                        new_links_found += 1
            except:
                continue
        
        if new_links_found > 0:
            print(f"ğŸ“° TÃ¬m tháº¥y {new_links_found} bÃ i má»›i sau scroll {i+1}")

        # Kiá»ƒm tra xem Ä‘Ã£ scroll háº¿t chÆ°a
        if scroll_position >= driver.execute_script("return document.body.scrollHeight"):
            print("ğŸ”š ÄÃ£ scroll háº¿t trang, dá»«ng")
            break

    print(f"âœ… Found {len(links)} news in order from top to bottom.")
    return links

def extract_article(driver, url):
    try:
        driver.get(url)
        time.sleep(3)
        soup = BeautifulSoup(driver.page_source, "html.parser")

        title_tag = soup.select_one("div.mt-3.mb-5.text-3xl.font-semibold.leading-10")
        title = title_tag.get_text(strip=True) if title_tag else ""

        fuzzy_time = ""
        dt = None

        time_tag = soup.select_one("time[datetime]")
        if time_tag:
            fuzzy_time = time_tag.get_text(strip=True)
            try:
                raw_iso = time_tag.get("datetime") or time_tag.get("title")
                if raw_iso:
                    dt = parser.parse(raw_iso)
            except Exception as e:
                print(f"âš ï¸ Lá»—i parse ISO datetime: {e}")

        if not dt:
            fuzzy_tags = soup.select("span.text-gray-500")
            if fuzzy_tags:
                for tag in fuzzy_tags:
                    parts = tag.get_text(strip=True).split("|")
                    if len(parts) >= 1:
                        fuzzy_time = parts[-1].strip()  # Láº¥y pháº§n cuá»‘i (thá»i gian)

        content_div = soup.find("div", id="post_content")
        content = ""
        if content_div:
            paragraphs = content_div.find_all("p")
            content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))

        # ğŸ”— NEW: láº¥y link gá»‘c (náº¿u cÃ³)
        source_link = extract_source_link_from_post(soup)

        try:
            ai_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(., 'TÃ³m táº¯t tin tá»©c báº±ng AI')]"))
            )
            ai_button.click()
            time.sleep(10)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            ai_summary_tag = soup.select_one("div.italic:not(.font-bold)")
            ai_summary = ai_summary_tag.get_text(strip=True) if ai_summary_tag else ""
        except Exception as e:
            print(f"âš ï¸ AI summary lá»—i: {e}")
            ai_summary = ""

        return {
            "title": title,
            "content": content,
            "link": url,
            "ai_summary": ai_summary,
            "fuzzy_time": fuzzy_time,
            "source_link": source_link,  # âœ… NEW
        }
    except Exception as e:
        print(f"âŒ Lá»—i khi crawl bÃ i viáº¿t: {url} ({e})")
        return {}

def insert_to_supabase(db_manager, table_name, data):
    """Wrapper function Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i code cÅ© - sá»­ dá»¥ng hÃ m chung"""
    # Táº¡o date parser function cho FireAnt
    def fireant_date_parser_wrapper(date_str):
        dt = parse_fuzzy_datetime(date_str, 2025)
        return format_datetime_for_db(dt) if dt else None
    
    return insert_article_to_database(db_manager, table_name, data, fireant_date_parser_wrapper)

def crawl_fireant(stock_code="FPT", table_name="FPT_News", db_manager=None):
    """Crawl FireAnt vá»›i logic tá»‘i Æ°u - chá»‰ check duplicate tá»« DB"""
    start_time = time.time()
    
    # Táº¡o db_manager náº¿u khÃ´ng Ä‘Æ°á»£c truyá»n vÃ o
    if db_manager is None:
        db_manager = get_database_manager()
    
    print("\n===============================================================================")
    print(f"ğŸ” Äang kiá»ƒm tra database cho stock: {stock_code}")
    existing_links = get_recent_links_from_db(db_manager, table_name, 100)
    print(f"ğŸ“Š TÃ¬m tháº¥y {len(existing_links)} bÃ i viáº¿t trong DB (100 bÃ i gáº§n nháº¥t)")

    driver = setup_driver()
    article_links = scroll_and_collect_links(driver, stock_code=stock_code)
    
    # Chá»‰ lá»c bá» nhá»¯ng link Ä‘Ã£ cÃ³ trong DB Ä‘á»ƒ trÃ¡nh crawl trÃ¹ng
    links_to_crawl = [link for link in article_links if link not in existing_links]
    
    if len(links_to_crawl) > 0:
        print(f"ğŸ¯ Crawl {len(links_to_crawl)} bÃ i viáº¿t má»›i (bá» qua {len(article_links) - len(links_to_crawl)} bÃ i Ä‘Ã£ cÃ³)")
    else:
        print(f"ğŸ“° No new news - táº¥t cáº£ {len(article_links)} bÃ i Ä‘á»u Ä‘Ã£ cÃ³ trong DB")

    current_year = 2025
    new_articles = 0
    crawled_count = 0
    duplicate_count = 0
    
    for idx, link in enumerate(links_to_crawl):
        try:
            print(f"ï¿½ [{idx+1}/{len(links_to_crawl)}] {link}")
            raw_data = extract_article(driver, link)

            dt = parse_fuzzy_datetime(raw_data.get("fuzzy_time", ""), current_year)
            raw_data["date"] = format_datetime_obj(dt) if dt else ""

            success = insert_to_supabase(db_manager, table_name, raw_data)
            if success:
                new_articles += 1
                print(f"âœ… ÄÃ£ lÆ°u bÃ i viáº¿t: {raw_data.get('title', '')[:50]}...")
            else:
                duplicate_count += 1
                # Chá»‰ hiá»ƒn thá»‹ 3 duplicate Ä‘áº§u tiÃªn Ä‘á»ƒ trÃ¡nh spam
                if duplicate_count <= 3:
                    print(f"âš ï¸  Duplicate title - skipped: {raw_data.get('title', '')[:50]}...")
                elif duplicate_count == 4:
                    print(f"âš ï¸  ... vÃ  {len(links_to_crawl) - idx - 1} duplicates khÃ¡c (khÃ´ng hiá»ƒn thá»‹)")
            
            crawled_count += 1
            time.sleep(1)  # Delay between requests
            
        except Exception as e:
            print(f"âŒ Lá»—i láº¥y bÃ i {link}: {e}")
            continue
    
    driver.quit()
    
    # TÃ­nh toÃ¡n káº¿t quáº£
    end_time = time.time()
    duration = end_time - start_time
    
    return {
        'stock_code': stock_code,
        'duration': duration,
        'total_found': len(article_links),
        'crawled_count': crawled_count,
        'new_articles': new_articles,
        'stopped_early': False
    }

def scroll_and_collect_general_articles(driver):
    """DEPRECATED - Function removed as general news crawling is no longer needed"""
    print("âš ï¸ General news crawling has been disabled")
    return []

def crawl_fireant_general(table_name="General_News", db_manager=None):
    """DEPRECATED - General news crawling disabled"""
    print("âš ï¸ General news crawling has been disabled - only stock-specific news are processed")
    return {
        'type': 'General',
        'duration': 0,
        'total_found': 0,
        'crawled_count': 0,
        'new_articles': 0,
        'stopped_early': False
    }

def main_fireant():
    """Main function vá»›i dashboard timing vÃ  thá»‘ng kÃª"""
    start_time = time.time()
    start_time_str = datetime.now().strftime("%H:%M:%S")
    
    print("\n" + "â•" * 60)
    print("ğŸš€ FIREANT CRAWLER DASHBOARD".center(60))
    print(f"â° Started: {start_time_str}".center(60))
    print("â•" * 60)

    db_manager = get_database_manager()
    results = []
    
    # Crawl cho tá»«ng stock code
    for i, code in enumerate(STOCK_CODES, 1):
        print(f"\nğŸš€ Processing Stock [{i}/{len(STOCK_CODES)}]: {code}")
        table_name = get_table_name(stock_code=code)
        print(f"ğŸ“‹ Save to: {table_name}")
        
        result = crawl_fireant(stock_code=code, table_name=table_name, db_manager=db_manager)
        results.append(result)
        
        # Delay between stocks
        if i < len(STOCK_CODES):
            time.sleep(3)
    
    print(f"\nï¿½ General News crawling has been disabled - only processing 4 stock codes: {STOCK_CODES}")

    db_manager.close_connections()
    
    # Hiá»ƒn thá»‹ dashboard káº¿t quáº£
    end_time = time.time()
    total_duration = end_time - start_time
    total_found = sum(r['total_found'] for r in results)
    total_crawled = sum(r['crawled_count'] for r in results)
    total_new = sum(r['new_articles'] for r in results)
    
    print("\n" + "â•" * 60)
    print("ğŸ‰ CRAWLING COMPLETED - RESULTS".center(60))
    print("â•" * 60)
    
    # Table header
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Type                     â”‚ Time   â”‚ Status       â”‚ Saved Articles    â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    # Table rows
    for result in results:
        type_name = result.get('stock_code', result.get('type', 'Unknown')).ljust(24)[:24]
        duration = result['duration']
        new_count = result['new_articles']
        stopped = result['stopped_early']
        
        # Status Ä‘Æ¡n giáº£n
        status = "No new news" if new_count == 0 else "New news"
        # Saved Articles = sá»‘ bÃ i thá»±c sá»± Ä‘Æ°á»£c lÆ°u
        results_text = f"{new_count} saved"
        
        print(f"â”‚ {type_name} â”‚ {duration:>6.1f}s â”‚ {status:<12} â”‚ {results_text:<17} â”‚")
    
    # Table footer
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # Summary
    print("\n" + "â•" * 60)
    print("ğŸ“Š SUMMARY FIREANT CRAWLING".center(60))
    print("â”€" * 60)
    print(f"â±ï¸  Total Time      : {total_duration:.1f}s ({total_duration/60:.1f} minutes)")
    print(f"ğŸ“Š Total Found     : {total_found} articles")
    print(f"ğŸ¯ Total Crawled   : {total_crawled} articles")
    print(f"âœ… Total New       : {total_new} articles")
    print("â•" * 60)
    print("ğŸ¯ FIREANT CRAWLING COMPLETED!")
    print("â•" * 60)

if __name__ == "__main__":
    main_fireant()

